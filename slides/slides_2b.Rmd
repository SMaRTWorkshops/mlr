---
title: '<span style="font-size:48pt;">Feature Engineering Recipes</span>'
subtitle: 'üë∑  üßë‚Äçüç≥  üç≥ '
author: 'Machine Learning in R<br /><i>SMaRT Workshops</i>'
date: 'Day 2B &emsp; &emsp; Jeffrey Girard'
output:
  xaringan::moon_reader:
    css: [../css/xaringan-themer.css, ../css/styles.css]
    nature:
      slideNumberFormat: "%current% / %total%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
      navigation:
        scroll: false
    self_contained: true
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  fig.showtext = TRUE,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE,
  collapse = TRUE
)
```

```{r xaringanExtra, echo=FALSE}
xaringanExtra::use_tachyons()
xaringanExtra::use_clipboard()
xaringanExtra::use_tile_view()
```

```{r packages, echo=FALSE, message=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
library(countdown)
library(patchwork)
library(xaringanthemer)
library(tidymodels)
tidymodels_prefer()
```


class: inverse, center, middle
# Recipes

---
class: onecol

## Feature Engineering
.left-column.pv3[
```{r engineer, echo=FALSE}
include_graphics("../figs/engineer.jpg")
```
]
.right-column.lh-copy[
**Prepare the predictors for analysis**
- *Extract* predictors
- *Transform* predictors
- *Re-encode* predictors
- *Combine* predictors
- *Reduce* predictor dimensionality
- *Impute* missing predictor values
- *Select* and drop predictors
]


---

class: onecol
## Recipes for Feature Engineering

.left-column.pv3[
```{r, echo=FALSE}
include_graphics("../figs/chef.jpg")
```

]

.right-column[
We will be learning the {recipes} package from {tidymodels}

1. Initiate a recipe using `recipe()`

2. Declare variable roles using `update_role()` (or a formula)

3. Include one or more preprocessing steps using `step_*()`

4. Add the recipe to our workflow using `add_recipe()`

5. Fit the model and {tidymodels} will take care of the rest!
]

---

## Two approaches to tidying

#### The old way (mutate the original data frame)

```{r, message=FALSE}
# Load and tidy data
titanic <- 
  read_csv("https://tinyurl.com/mlr-titanic") %>%
  mutate( #<<
    survived = factor(survived),
    pclass = factor(pclass),
    sex = factor(sex)
  )
```

--

#### The new way (deal with it later using recipes)

```{r}
# Load data without tidying
titanic <- read_csv("https://tinyurl.com/mlr-titanic")
```

---

## Setting up the data splits

```{r}
# Create data splits, stratified by fare

set.seed(2022)
fare_split <- initial_split(data = titanic, prop = 0.8, strata = 'fare')
fare_train <- training(fare_split)
fare_test <- testing(fare_split)
```

```{r}
# Set up model (linear regression using lm)

lm_model <- 
  linear_reg() %>% 
  set_mode("regression") %>% 
  set_engine("lm")
```


---

## Initiating a recipe

```{r}
# Option 1: Set roles using a formula

fare_recipe <- 
  recipe(fare_train, formula = fare ~ pclass + sex + age + sibsp + parch)
```

--

```{r}
# Option 2: Set roles using update_role()

fare_recipe <-
  recipe(fare_train) %>% 
  update_role(fare, new_role = "outcome") %>% 
  update_role(pclass:parch, new_role = "predictor") %>% 
  update_role(survived, new_role = "ignore")
```

.footnote[*Note.* I personally prefer `update_roles()` because it enables tidy selection and "ignored" variables.]

---
class: onecol
## Including preprocessing steps

```{r}
# New recipe-based version

fare_recipe <- 
  fare_recipe %>% 
  step_mutate( #<<
    survived = factor(survived, levels = c(1, 0)),
    pclass = factor(pclass),
    sex = factor(sex)
  )
```

The benefit of making this a recipe step (instead of applying it to the original data)...

...is that it will now affect *any data* that you apply the recipe/workflow to!

---

## Using a recipe

```{r}
# Add the recipe (instead of the formula) to the workflow

fare_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(fare_recipe) #<<
```

--

```{r}
# Fit the model with this workflow (exactly as before)

fare_fit <- last_fit(fare_wflow, fare_split)
```

---
class: twocol
## Common Steps

.pull-left.lh-copy[
- **Calculate and transform:**<br />
  `step_mutate()`, ...
- **Categorical predictors:**<br />
  `step_dummy()`
- **Center and rescale:**<br />
  `step_normalize()`, ...
- **Non-normality:**<br />
  `step_YeoJohnson()`, ...
- *Interactions:*<br />
  `step_interact()`
- *Non-linearity:*<br />
  `step_poly()`, ...
]

.pull-right.lh-copy[
- **Missing values:**<br />
  `step_naomit()`, `step_impute_*()`, ...
- **Near-zero variance:**<br />
  `step_nzv()`
- **Multicollinearity:**<br />
  `step_corr()`
- **Linear combinations:**<br />
  `step_lincomb()`
- *Drop variables:*<br />
  `step_rm()`
- *Dimensionality reduction:*<br />
  `step_pca()`, ...
]

---
class: onecol
## Calculate and transform

-   Some variables will need to be calculated from existing values and variables

  -   You may choose to score an instrument from item-level data

  -   You may choose to encode a predictor as the ratio of two values

  -   You may choose to calculate sums, means, counts, proportions, etc.

.pv1[
-   We can use `step_mutate()` for these purposes as well
]

---
## Calculate and transform

```{r}
cp_recipe <- 
  fare_recipe %>% 
  step_mutate( #<<
    numfamily = sibsp + parch, #<<
    fa_ratio = fare / age #<<
  ) #<<
```

--

```{r echo=FALSE}
cp_recipe %>% 
  prep() %>% 
  bake(NULL) %>% 
  head(100) %>% 
  kable() %>% 
  kable_styling() %>% 
  scroll_box(height="260px")
```


---
class: onecol
## Categorical predictors

Categorical predictors can be re-encoded into multiple binary (0 or 1) predictors

In `titanic`, `sex` is *female* or *male* and `pclass` is *1*, *2*, or *3*

With dummy coding, you end up with $g-1$ binary predictors $(g$ is number of levels)

--

.pull-left.pad-table[
.center.imp[Two Levels]

| sex    | sex_male | 
|:------ |:--------:|
| female | 0        |
| male   | 1        |
]

--

.pull-right.pad-table[
.center.imp[Three Levels]

| pclass | pclass_X2 | pclass_X3 |
|:------ |:---------:|:---------:|
| 1      | 0         | 0         |
| 2      | 1         | 0         |
| 3      | 0         | 1         |

]

---
## Categorical predictors

```{r}
dc_recipe <- 
  fare_recipe %>% 
  step_dummy(sex, pclass) #<<
```

--

```{r echo=FALSE}
dc_recipe %>% 
  prep() %>% 
  bake(NULL) %>%
  head(100) %>% 
  kable() %>% 
  kable_styling() %>% 
  scroll_box(height="300px")
```

---
class: onecol
## Center and rescale

Predictors with vastly different means and SDs can cause problems for some algorithms

--

.imp[Normalizing] a predictor sets its mean to $0.0$ and its SD to $1.0$

- This is accomplished by subtracting the mean and then dividing by the SD

- This is also sometimes called "standardizing" or $z$-scoring the predictor

--

```{r}
# Normalize the age variable
nr_recipe <- 
  fare_recipe %>% 
  step_normalize(age) #<<
```

.footnote[*Note.* {tidymodels} will use the *training set's* mean and SD to avoid data leakage from the testing set.]

---
## Center and rescale

```{r normalizing, echo=FALSE}
x <- titanic$age
x_n <- (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)

p1 <- ggplot(tibble(x), aes(x)) + geom_density(na.rm = TRUE) + 
  annotate(
    "label", 
    x = mean(x, na.rm = TRUE), 
    y = 0.0055, 
    label = glue::glue("M={round(mean(x, na.rm = TRUE), 1)}, SD={round(sd(x, na.rm = TRUE), 1)}"),
    size = 5
    ) +
  labs(x = "age", y = "density") +
  theme_xaringan(text_font_size = 16, title_font_size = 18,
                 css_file = "../css/xaringan-themer.css") +
  theme(
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )

p2 <- ggplot(tibble(x_n), aes(x_n)) + geom_density(na.rm = TRUE) + 
  annotate(
    "label", 
    x = mean(x_n, na.rm = TRUE), 
    y = 0.075, 
    label = sprintf("M=%.1f, SD=%.1f", mean(x_n, na.rm = TRUE), sd(x_n, na.rm = TRUE)),
    size = 5
    ) +
  labs(x = "age_normalized", y = NULL) +
  theme_xaringan(text_font_size = 16, title_font_size = 18,
                 css_file = "../css/xaringan-themer.css") +
  theme(
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
p1 | p2
```

.footnote[The mean is now 0.0 and the SD is now 1.0, but the shape of the distribution is unchanged.]

---
class: onecol
## Non-normality

A .imp[skewed] distribution is one that is not symmetric (i.e., it has a "heavy tail")

A .imp[bounded] distribution is one that cannot go beyond certain boundary values

```{r skew, echo=FALSE, out.width='90%'}
n <- 1e5
skew_ex <- tibble(
  x = c(
    rbeta(n, 1, 8), 
    rbeta(n, 9, 9),
    rbeta(n, 8, 1)
  ),
  type = factor(
    rep(c("Positively Skewed", "Symmetrical", "Negatively Skewed"), each = n),
    levels = c("Positively Skewed", "Symmetrical", "Negatively Skewed")
  )
)
ggplot(skew_ex, aes(x = x, fill = type, linetype = type)) + 
  geom_density(size = 1.25, alpha = 0.4, color = "grey40") + 
  scale_fill_brewer(palette = "BrBG") +
  scale_linetype_manual(values = c("solid", "dotted", "solid"), 
                        guide = "legend") +
  labs(x = NULL, fill = NULL, linetype = NULL) +
  theme_xaringan(text_font_size = 16, title_font_size = 18,
                 css_file = "../css/xaringan-themer.css") +
  theme(
    legend.position = "bottom",
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
```

---
class: onecol
## Non-normality
Specific transformations (e.g., log, inverse, logit) can help address specific issues

The Box-Cox and Yeo-Johnson approaches employ **families of transformations**

Box-Cox cannot be applied to negative or zero values, but .imp[Yeo-Johnson] can

<br />

$$x_{(yj)}^\star=\begin{cases}((x+1)^\lambda-1)/\lambda & \text{if } \lambda\ne0, x\ge0 \\
\log(x+1) & \text{if } \lambda=0, x\ge0 \\
-[(-x+1)^{2-\lambda}-1)]/(2-\lambda) & \text{if } \lambda\ne2, x<0 \\
-\log(-x+1) & \text{if } \lambda=2, x<0
\end{cases}$$

.footnote[
*Note.* The $\lambda$ parameter will be estimated from the *training set* only.
]

---
## Non-normality

```{r}
yj_recipe <- 
  fare_recipe %>% 
  step_YeoJohnson(fare) #<<
```

--

```{r yjfare, echo = FALSE, fig.height=3}
yj_t <- yj_recipe %>% prep() %>% bake(NULL)
p1 <- 
  ggplot(titanic, aes(x = fare)) + 
  geom_density(size = 1.25, alpha = 0.4, color = "grey40", 
               fill = "#91bfdb", na.rm = TRUE) + 
  labs(x = "fare", y = "density") +
  theme_xaringan(text_font_size = 16, title_font_size = 18,
                 css_file = "../css/xaringan-themer.css") +
  theme(
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
p2 <- 
  ggplot(yj_t, aes(x = fare)) + 
  geom_density(size = 1.25, alpha = 0.4, color = "grey40", 
               fill = "#91bfdb", na.rm = TRUE) + 
  labs(x = "Transformed fare (Yeo-Johnson)", y = NULL) +
  theme_xaringan(text_font_size = 16, title_font_size = 18,
                 css_file = "../css/xaringan-themer.css") +
  theme(
    panel.grid.minor = element_blank(), 
    panel.background = element_rect(fill = "white")
  )
p1 | p2
```

---

class: onecol
## Shortcuts and conveniences

-   Within recipes, there are shortcuts for selecting multiple variables

  -   **Roles:** `all_outcomes()`, `all_predictors()`
  
  -   **Types:** `all_numeric()`, `all_nominal()`
  
  -   **Both:** `all_numeric_outcomes()`, `all_numeric_predictors()`, <br />
  `all_nominal_outcomes()`, `all_nominal_predictors()`

--

-   We can also chain together multiple sequential steps via pipes<sup>1</sup>

--

```{r, eval=FALSE}
fare_recipe %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors())
```
  
  
.footnote[[1] The steps will be applied in *sequential* order, so build your recipe pipeline thoughtfully.]
---
class: onecol
## Missing values

-   It is not uncommon for outcomes and predictors to have missing values

--

-   If we have marked these values correctly as `NA`, we can "impute" them

  -   *To impute a missing value is to fill it in with a plausible value*

  -   Imputation can be simple (e.g., fill in the mean, median, or mode)
  
  -   Imputation can be complex (e.g., build a model to predict the value)
  
  -   We will use a linear imputation model via `step_impute_linear()`
  
--

-   Imputing predictor values is fine when you are using cross-validation

  -   But omit observations with missing outcome values using `step_naomit()`

.footnote[*Note.* You can specify which variables to use in your imputation model via the `impute_with` argument.]

---

## Missing values

```{r}
na_recipe <- 
  fare_recipe %>% 
  step_naomit(fare) %>% #<<
  step_impute_linear(age) #<<
```

.footnote[*Note.* The default is to include all other predictor variables in the imputation model.]

--

```{r echo=FALSE}
na_recipe %>% 
  prep() %>% 
  bake(NULL) %>%
  head(100) %>% 
  kable() %>% 
  kable_styling() %>% 
  scroll_box(height="300px")
```

---
class: onecol
## Near-zero variance
.imp[Zero variance predictors] take on only a single value in the sample

- These predictors are **uninformative** and may lead to **modeling problems**

--

.pv1[
.imp[Near-zero variance predictors] take on only a few unique values with low frequencies

- These predictors can easily become zero-variance predictors during resampling
]

--

<p style="padding-top:25px;">For many algorithms, we want to <b>detect</b> and <b>remove</b> both types of predictors</p>

(This may not be necessary for algorithms with built-in *feature selection*)

---
## Near-zero variance

```{r}
nzv_recipe <- 
  fare_recipe %>% 
  step_mutate(
    species = "homo sapiens",  # will have zero variance
    over70 = age > 70 # will have near-zero variance
  ) %>% 
  step_nzv(all_predictors()) #<<
```

--

```{r echo=FALSE}
nzv_recipe %>% 
  prep() %>% 
  bake(NULL) %>%
  head(100) %>% 
  kable() %>% 
  kable_styling() %>% 
  scroll_box(height="200px")
```

.footnote[*Note.* The `species` and `over70` variables were removed for having near-zero variance.]

---
class: onecol
## Redundancy

.imp[Highly correlated predictors] can lead to problems for some algorithms/procedures

- The model has to randomly choose between the predictors, leading to **instability**
- Model predictions may be fine, but model **interpretation** will often be obfuscated
- The cutoff for "problematically high" correlations varies (e.g., 0.5 to 0.9 or higher)

--

.pv1[
Predictors that are .imp[linear combinations] of other predictors are similarly problematic

- Occurs if a predictor variable can be predicted from the other predictor variables
- (This is why dummy coding creates $g-1$ instead of $g$ binary variables)
]

--

<p style="padding-top:25px;">For many algorithms, we want to <b>detect</b> and <b>remove</b> redundant predictors</p>

(This may not be necessary for algorithms with *regularization* or *feature selection*)

---
## Redundancy

```{r}
re_recipe <- 
  fare_recipe %>% 
  step_impute_linear(age) %>% 
  step_mutate(
    wisdom = age / 100, # high correlation
    nfamily = sibsp + parch # linear combination
  ) %>% 
  step_corr(all_numeric_predictors(), threshold = 0.9) %>% #<<
  step_lincomb(all_numeric_predictors()) #<<
```

--

```{r echo=FALSE}
re_recipe %>% 
  prep() %>% 
  bake(NULL) %>%
  head(100) %>% 
  kable() %>% 
  kable_styling() %>% 
  scroll_box(height="160px")
```

.footnote[*Note.* Here `age` was dropped due to its high correlation with `wisdom`, and `nfamily` was dropped due to being a linear combination of `sibsp` and `parch`.]

---

## Live Coding: Putting it all together

.scroll.h-0l[
```{r, eval=FALSE}
# Load data (without tidying it - we'll do that in the recipe)
titanic <- read_csv("https://tinyurl.com/mlr-titanic")

# Create data splits, stratified by fare

set.seed(2022)
fare_split <- initial_split(data = titanic, prop = 0.8, strata = 'fare')
fare_train <- training(fare_split)
fare_test <- testing(fare_split)

# Set up model (linear regression using lm)

lm_model <- 
  linear_reg() %>% 
  set_mode("regression") %>% 
  set_engine("lm")

# Prepare recipe

fare_recipe <- 
  recipe(fare_train) %>% 
  update_role(fare, new_role = "outcome") %>% 
  update_role(pclass:parch, new_role = "predictor") %>% 
  update_role(survived, new_role = "ignore") %>% 
  step_naomit(fare) %>% 
  step_mutate(
    pclass = factor(pclass),
    sex = factor(sex)
  ) %>% 
  step_dummy(all_nominal_predictors()) %>%
  step_impute_linear(age) %>%
  step_nzv(all_predictors()) %>%
  step_corr(all_numeric_predictors()) %>%
  step_lincomb(all_numeric_predictors()) %>% 
  step_normalize(sibsp, parch, age)

# Prepare workflow

fare_wflow <-
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(fare_recipe)

# Set up metric set

fare_ms <- metric_set(rmse, rsq, huber_loss, ccc)

# Fit recipe and model to training set and calculate metrics in testing set

fare_fit <- 
  last_fit(fare_wflow, fare_split, metrics = fare_ms)

# Examine metrics

collect_metrics(fare_fit)
```
]

---
class: inverse, center, middle
# Time for a Break!
```{r countdown, echo=FALSE}
countdown(
  minutes = 60, 
  seconds = 0, 
  right = "33%", 
  left = "33%",
  bottom = "15%",
  color_background = "white",
  color_text = "black",
  color_running_background = "white",
  color_running_text = "black",
  warn_when = 120
)
```
